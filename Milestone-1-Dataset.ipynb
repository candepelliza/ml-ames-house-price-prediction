{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone-1: Data description and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook corresponds to the first stage of the Machine Learning final project, as part of the Copernicus Master in Digital Earth, in the Data Science track at UBS.\n",
    "\n",
    "The project is focused on properties' price prediction for he city of Ames, Iowa, Us. The different steps and stages of the work are developed and documented in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Literal description of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset collection\n",
    "This project will be centered around properties' prices prediction, using a dataset from the city of Ames, Iowa, United States. This dataset was obtained from the [House Price - Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data) from the website Kaggle. In there, the dataset is made available for the means of the competition, but with rights allowing to use it for academic purposes, as long as it is not made publicly available ([see section 7.A of copetition rules](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/rules#7.-competition-data.)).\n",
    "\n",
    "##### Meaning of the dataset\n",
    "The Ames' properties prices contains .. records of individual residential properties sold in Ames, IA from 2006 to 2010. Each record contains the selling price, and a group of 81 descriptive variables, related to different characteristics of the property, their environment and the selling transaction itself. All these attributes provide a detailed description of the operations and, together with the big amount of records, position the dataset as an excellent input for developing, training and testin diverse sources of Machine Learning models. \n",
    "Moreover, while the registers do not contain the exact location of the properties, they do include characteristics related to location and environment, such as neighbourhood, proximity to avenues and zonning. For this reason, the dataset is extremely interesting in geographic terms, as it allows us to analyse the impact of urban-spatial factors on property values.\n",
    "\n",
    "##### Explanatory and Response variables\n",
    "\n",
    "The analyzed response variable is the property selling price, which is expressed in American Dollars "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Visual description of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Statistical description of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.A. Descriptive statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.B. Potential correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.C. Potential pre-processing (we can probably move this to before visual description)?\n",
    "\n",
    "1. To manage missing values, \n",
    "- We cannot drop all rows with any missing value because there are some columns with very few non-null values (e.g. PoolQC -7 non null, MiscFeature - 54 non null, Alley - 91 non null, and Fence - 281 non null). So dropping these rows (on 'any' columns) would mean losing a lot of data.\n",
    "\n",
    "- To handle this, we first drop the columns that have missing values for more than half of the training sample. This leaves us with xxxx columns of explanatory variable.\n",
    "\n",
    "- Then, for numeric variables, we replace missing values with the mean of the column. (This is because, at this stage too, we cannot drop rows with missing values because we would lose a lot of data. For example, the column 'LotFrontage' has only 770 non null values (train and test combined)).\n",
    "\n",
    "- For categorical variables, we replace missing values with the mode of the column.\n",
    "\n",
    "3. To normalize the data \n",
    "- We perform mean centring so that, for each numeric column mean is 0 and standard deviation is 1. \n",
    "\n",
    "4. To use categorical explanatory variables, \n",
    "- For binary variables, we use 0 and 1 to represent the two categories.\n",
    "- For ordinal variables, we rank the categories and use the ranks as the values.\n",
    "- For nominal variables, we use one-hot encoding to convert categorical variables into dummy/indicator variables. \n",
    "\n",
    "\n",
    "5. To add additional explanatory variables\n",
    "- Summing up floor space across floors as a new explanatory variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Proposed evaluation protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.A. Protocol for perfomance assessment, tuning of hyperparameter, splitting of data (data leaks  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preventing train-test data leaks\n",
    "1. Ensuring pre processing steps such as filling with mean/mode, standardisation and [encoding](https://community.databricks.com/t5/machine-learning/do-one-hot-encoding-ohe-before-or-after-split-data-to-train-and/td-p/17888#:~:text=%22If%20you%20perform%20the%20encoding,scores%20but%20poor%20in%20deployment) happens after train-test split \n",
    "2. Fixing train and test samples while tuning hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.B. Evaluation metric\n",
    "\n",
    "Since our problem is defined as a regression problem, we will use the Root Mean Squared Error (RMSE) as the main metric. This is because RMSE is a good measure of how far the predicted values are from the actual value. It is also a good measure of how large the residuals are. In ordered to allow for a standardised comparison, we will calculate R2 score as well. R2 gives a measure of how much of the variance in the data is explained by the model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
